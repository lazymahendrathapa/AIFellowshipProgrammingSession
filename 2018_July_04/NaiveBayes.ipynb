{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use Naive Bayes Classifier for sms spam detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spam.csv', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               Text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['v1', 'v2']].rename(columns={'v1': 'Label', 'v2': 'Text'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the labels to numerical values\n",
    "df['Label'] = df.Label.map({'ham': 0, 'spam': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5572"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset as: training (3572), validation (1000), test (1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[:-2000]\n",
    "val_df = df.iloc[-2000:-1000]\n",
    "test_df = df.iloc[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fec3efa78d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12,4))\n",
    "train_df.Label.value_counts().plot.bar(ax=axes[0], title='Training')\n",
    "val_df.Label.value_counts().plot.bar(ax=axes[1], title='Validation')\n",
    "test_df.Label.value_counts().plot.bar(ax=axes[2], title='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should probably shuffle the dataset before splitting but it looks like the classes are evenly distributed even when directly splitting. So, we let it be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_metrics(actual, preds):\n",
    "    print('Accuracy score: {}'.format(accuracy_score(actual, preds)))\n",
    "    print('Precision score: {}'.format(precision_score(actual, preds)))\n",
    "    print('Recall score: {}'.format(recall_score(actual, preds)))\n",
    "    print('F1 score: {}'.format(f1_score(actual, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_df['Text'], train_df['Label']\n",
    "x_val, y_val = val_df['Text'], val_df['Label']\n",
    "x_test, y_test = test_df['Text'], test_df['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Baseline (All hams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As *Ham* is the more common class, the baseline could be a model that always predicts ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.865\n",
      "Precision score: 0.0\n",
      "Recall score: 0.0\n",
      "F1 score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yokai/anaconda3/envs/py3k/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/yokai/anaconda3/envs/py3k/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "preds = np.zeros_like(y_val)\n",
    "report_metrics(y_val, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation 1: Without removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "counts = vectorizer.fit_transform(x_train)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(counts, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.987\n",
      "Precision score: 0.9765625\n",
      "Recall score: 0.9259259259259259\n",
      "F1 score: 0.9505703422053232\n"
     ]
    }
   ],
   "source": [
    "val_counts = vectorizer.transform(x_val)\n",
    "preds = classifier.predict(val_counts)\n",
    "report_metrics(y_val, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation 2: Removing Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "counts = vectorizer.fit_transform(x_train)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(counts, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.986\n",
      "Precision score: 0.9763779527559056\n",
      "Recall score: 0.9185185185185185\n",
      "F1 score: 0.9465648854961832\n"
     ]
    }
   ],
   "source": [
    "val_counts = vectorizer.transform(x_val)\n",
    "preds = classifier.predict(val_counts)\n",
    "report_metrics(y_val, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like not removing stop words results in higher F1 score. Strange, but we go with that.\n",
    "\n",
    "Also, we don't perform any other tuning thus taking this model as the best one on validation and moving on\n",
    "to test reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Test Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train on the full available training set i.e training + validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "counts = vectorizer.fit_transform(np.concatenate([x_train, x_val]))\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(counts, np.concatenate([y_train, y_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.985\n",
      "Precision score: 0.9402985074626866\n",
      "Recall score: 0.9473684210526315\n",
      "F1 score: 0.9438202247191011\n"
     ]
    }
   ],
   "source": [
    "test_counts = vectorizer.transform(x_test)\n",
    "preds = classifier.predict(test_counts)\n",
    "report_metrics(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. From Scratch (Sort of)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will build our own Naive Bayes Classifier from \"scratch\".\n",
    "\n",
    "We will directly go for the final test. No tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_df = pd.concat([train_df, val_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts for Ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Go until jurong point, crazy.. Available only ...\n",
       "1                        Ok lar... Joking wif u oni...\n",
       "3    U dun say so early hor... U c already then say...\n",
       "4    Nah I don't think he goes to usf, he lives aro...\n",
       "6    Even my brother is not like to speak with me. ...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_texts = train_full_df[train_full_df.Label == 0]['Text']\n",
    "ham_texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 1784),\n",
       " ('you', 1414),\n",
       " ('to', 1271),\n",
       " ('the', 918),\n",
       " ('a', 863),\n",
       " ('u', 717),\n",
       " ('and', 710),\n",
       " ('in', 644),\n",
       " ('my', 611),\n",
       " ('is', 592)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_counts = Counter()\n",
    "for text in ham_texts:\n",
    "    for word in text.split():\n",
    "        ham_counts[word.lower()] += 1\n",
    "ham_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts for Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "5     FreeMsg Hey there darling it's been 3 week's n...\n",
       "8     WINNER!! As a valued network customer you have...\n",
       "9     Had your mobile 11 months or more? U R entitle...\n",
       "11    SIX chances to win CASH! From 100 to 20,000 po...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_texts = train_full_df[train_full_df.Label == 1]['Text']\n",
    "spam_texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 559),\n",
       " ('a', 299),\n",
       " ('call', 272),\n",
       " ('your', 204),\n",
       " ('you', 203),\n",
       " ('the', 165),\n",
       " ('for', 159),\n",
       " ('free', 149),\n",
       " ('or', 149),\n",
       " ('2', 140)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_counts = Counter()\n",
    "for text in spam_texts:\n",
    "    for word in text.split():\n",
    "        spam_counts[word.lower()] += 1\n",
    "spam_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we need two things to compute the *posterior*: the *likelihood* and the *prior*. \n",
    "(We don't need to consider the *evidence*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's attempt to define the likehood calculator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_likelihood(tokens, counts):\n",
    "    \"\"\"Calculates likelihood for given tokens.\n",
    "    \n",
    "    Due to independence assumption, it's just the product of the\n",
    "    individual probabilities.\n",
    "    \"\"\"\n",
    "    total_count = sum(counts.values())\n",
    "    l = 1.0\n",
    "    for t in tokens:\n",
    "        l *= counts[t.lower()] / total_count\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has two main issues.\n",
    "\n",
    "First issue: we get very small numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4602999740832923e-05"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_likelihood(['Free', 'entry'], spam_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, if one of the tokens weren't in the corpus, everything ends up being zero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_likelihood(['Free', 'food'], spam_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle these issues, we use... math!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_likelihood(tokens, counts):\n",
    "    \"\"\"Calculates log likelihood for given tokens.\n",
    "    \n",
    "    Also uses Laplacian smoothing to handle tokens with zero counts.\n",
    "    \n",
    "    Note: The product over the probabilities has become sum over log probabilities.\n",
    "    The denominator is the same for all tokens. We can deduct it at the end for \n",
    "    each token. So, len(tokens) number of deductions.\n",
    "    \"\"\"\n",
    "    total_count = sum(counts.values())\n",
    "    vocab_size = len(counts)\n",
    "    l = 0.0\n",
    "    for t in tokens:\n",
    "        l += math.log(counts[t.lower()] + 1)\n",
    "    l = l - len(tokens) * math.log(total_count + vocab_size)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.493576076218668"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_log_likelihood(['Free', 'entry'], spam_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-14.584618529576984"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_log_likelihood(['Free', 'food'], spam_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The priors are the class probabilities (in the training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1342957130358705\n",
      "0.8657042869641295\n"
     ]
    }
   ],
   "source": [
    "spam_prior = len(spam_texts) / len(train_full_df)\n",
    "print(spam_prior)\n",
    "ham_prior = len(ham_texts) / len(train_full_df)\n",
    "print(ham_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamClassifier:\n",
    "    def __init__(self, spam_prior, ham_prior, spam_counts, ham_counts):\n",
    "        self.spam_prior = spam_prior\n",
    "        self.ham_prior = ham_prior\n",
    "        self.spam_counts = spam_counts\n",
    "        self.ham_counts = ham_counts\n",
    "    \n",
    "    def predict(self, text):\n",
    "        spam_posterior = math.exp(calculate_log_likelihood(text.split(), self.spam_counts)) * self.spam_prior\n",
    "        ham_posterior = math.exp(calculate_log_likelihood(text.split(), self.ham_counts)) * self.ham_prior\n",
    "        return 1 if spam_posterior > ham_posterior else 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SpamClassifier(spam_prior, ham_prior, spam_counts, ham_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predict('How are you?'))\n",
    "print(classifier.predict('Free lunch!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.935\n",
      "Precision score: 0.6752577319587629\n",
      "Recall score: 0.9849624060150376\n",
      "F1 score: 0.8012232415902141\n"
     ]
    }
   ],
   "source": [
    "test_preds = [classifier.predict(text) for text in x_test]\n",
    "report_metrics(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs much worse (0.80 F1) than Sklearn's classifier (0.94 F1) but as the accuracy shows, it's better than the baseline (0.93 vs 0.86)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
